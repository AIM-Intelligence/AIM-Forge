"""
Together AI Model - Together AIë¥¼ í†µí•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í˜¸ì¶œ
ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ LLM (Llama, Mixtral, CodeLlama ë“±) ì‚¬ìš© ê°€ëŠ¥
"""

import os
from together import Together
from typing import Optional


def RunScript(
    Start: bool = True,
    prompt: str = "Hi, how are you?",
    system_prompt: str = "You are a helpful assistant.",
    model: str = "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    # === ì‚¬ìš© ê°€ëŠ¥í•œ Together AI ëª¨ë¸ ëª©ë¡ (2025.01) ===
    # 
    # ğŸ¦™ Llama ì‹œë¦¬ì¦ˆ:
    #   meta-llama/Llama-4-Scout-17B-16E-Instruct
    #   meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
    #   meta-llama/Llama-3.3-70B-Instruct-Turbo (ë¬´ë£Œ ë²„ì „ë„ ìˆìŒ)
    #   meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo
    #   meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
    #   meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
    #   meta-llama/Meta-Llama-3-70B-Instruct-Turbo
    #   meta-llama/Meta-Llama-3-8B-Instruct
    #   meta-llama/Llama-3.2-3B-Instruct-Turbo
    #   meta-llama/Llama-3.2-1B-Instruct
    #
    # ğŸ‰ Qwen ì‹œë¦¬ì¦ˆ:
    #   Qwen/QwQ-32B (ì¶”ë¡  íŠ¹í™”)
    #   Qwen/Qwen3-235B-A22B-Instruct-2507-tput
    #   Qwen/Qwen3-235B-A22B-Thinking-2507
    #   Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8
    #   Qwen/Qwen2.5-72B-Instruct-Turbo
    #   Qwen/Qwen2.5-7B-Instruct-Turbo
    #   Qwen/Qwen2.5-Coder-32B-Instruct
    #   Qwen/Qwen2.5-VL-72B-Instruct (ë¹„ì „ ì§€ì›)
    #
    # ğŸŒŠ DeepSeek ì‹œë¦¬ì¦ˆ:
    #   deepseek-ai/DeepSeek-R1 (ìµœì‹  ì¶”ë¡  ëª¨ë¸)
    #   deepseek-ai/DeepSeek-R1-Distill-Llama-70B (ë¬´ë£Œ ë²„ì „ë„ ìˆìŒ)
    #   deepseek-ai/DeepSeek-R1-Distill-Qwen-14B
    #   deepseek-ai/DeepSeek-V3.1
    #   deepseek-ai/DeepSeek-V3
    #
    # ğŸŒŸ Mistral ì‹œë¦¬ì¦ˆ:
    #   mistralai/Mistral-Small-24B-Instruct-2501
    #   mistralai/Mixtral-8x7B-Instruct-v0.1
    #   mistralai/Mistral-7B-Instruct-v0.3
    #   mistralai/Mistral-7B-Instruct-v0.2
    #
    # ğŸ§  Cogito ì‹œë¦¬ì¦ˆ (ëŒ€ê·œëª¨):
    #   deepcogito/cogito-v2-preview-deepseek-671b
    #   deepcogito/cogito-v2-preview-llama-405B
    #   deepcogito/cogito-v2-preview-llama-109B-MoE
    #   deepcogito/cogito-v2-preview-llama-70B
    #
    # ğŸ­ Arcee AI ì‹œë¦¬ì¦ˆ:
    #   arcee-ai/maestro-reasoning (ì¶”ë¡  íŠ¹í™”)
    #   arcee-ai/virtuoso-large
    #   arcee-ai/coder-large
    #   arcee-ai/AFM-4.5B
    #
    # ğŸŒ™ ê¸°íƒ€ ì£¼ìš” ëª¨ë¸:
    #   moonshotai/Kimi-K2-Instruct (Kimi ëª¨ë¸)
    #   lgai/exaone-deep-32b (LG AI)
    #   lgai/exaone-3-5-32b-instruct
    #   google/gemma-3n-E4B-it (Google)
    #   openai/gpt-oss-120b (OpenAI ì˜¤í”ˆì†ŒìŠ¤)
    #   togethercomputer/MoA-1-Turbo (Together AI ìì²´)
    #
    # ğŸ¨ ì´ë¯¸ì§€ ìƒì„± (FLUX):
    #   black-forest-labs/FLUX.1.1-pro
    #   black-forest-labs/FLUX.1-schnell-Free (ë¬´ë£Œ)
    #   black-forest-labs/FLUX.1-dev
    #
    # ğŸ”Š ìŒì„±/ì˜¤ë””ì˜¤:
    #   openai/whisper-large-v3
    #   cartesia/sonic-2
    #
    # ğŸ” ì„ë² ë”©/ë¦¬ë­í‚¹:
    #   BAAI/bge-large-en-v1.5
    #   mixedbread-ai/Mxbai-Rerank-Large-V2
    #   Salesforce/Llama-Rank-V1
    #
    temperature: float = 0.7,
    max_tokens: int = 5000,
    api_key: Optional[str] = None,
) -> dict:
    """
    Together AIë¥¼ í†µí•´ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
        prompt: ì‚¬ìš©ì ì…ë ¥ í”„ë¡¬í”„íŠ¸
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ëª¨ë¸ ì—­í•  ì •ì˜)
        model: ì‚¬ìš©í•  ëª¨ë¸ (Together AI ëª¨ë¸ ì´ë¦„)
        temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.0-2.0)
        max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        api_key: Together API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)

    Returns:
        response: ëª¨ë¸ì˜ ì‘ë‹µ
        usage: í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
        model: ì‚¬ìš©ëœ ëª¨ë¸ëª…
    """

    if not prompt:
        return {"error": "í”„ë¡¬í”„íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

    # API í‚¤ í™•ì¸
    api_key = api_key or os.getenv("TOGETHER_API_KEY")
    if not api_key:
        return {"error": "Together API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}

    try:
        # Together í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = Together(api_key=api_key)

        # API í˜¸ì¶œ (OpenAI í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
            ],
            temperature=temperature,
            max_tokens=max_tokens,
        )

        # ì‘ë‹µ ì¶”ì¶œ
        result = response.choices[0].message.content
        usage = {
            "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
            "completion_tokens": response.usage.completion_tokens if response.usage else 0,
            "total_tokens": response.usage.total_tokens if response.usage else 0,
        }

        return {"response": result, "usage": usage, "model": model}

    except Exception as e:
        return {"error": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"}